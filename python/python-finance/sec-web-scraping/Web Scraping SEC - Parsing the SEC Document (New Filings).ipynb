{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing SEC Filing Documents - New Filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our libraries\n",
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to be used later\n",
    "\n",
    "The text is a mess, so we will need to normalize it. However, we can't rely on just the unicodedata library to help us. There are also windows_1252_characters that will provide some challenges. I found a solution on Stack Overflow that'll help us normalize the remaining portions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_windows_1252_characters(restore_string):\n",
    "    \"\"\"\n",
    "        Replace C1 control characters in the Unicode string s by the\n",
    "        characters at the corresponding code points in Windows-1252,\n",
    "        where possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "        except UnicodeDecodeError:\n",
    "            # No character at the corresponding code point: remove it.\n",
    "            return ''\n",
    "        \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab the Document Content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the url to specific html_text file\n",
    "new_html_text = r\"https://www.sec.gov/Archives/edgar/data/1166036/000110465904027382/0001104659-04-027382.txt\"\n",
    "\n",
    "# grab the response\n",
    "response = requests.get(new_html_text)\n",
    "\n",
    "# pass it through the parser, in this case let's just use lxml because the tags seem to follow xml.\n",
    "soup = BeautifulSoup(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a master dictionary to house all filings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary that will house all filings.\n",
    "master_filings_dict = {}\n",
    "\n",
    "# let's use the accession number as the key. This \n",
    "accession_number = '0001104659-04-027382'\n",
    "\n",
    "# add a new level to our master_filing_dict, this will also be a dictionary.\n",
    "master_filings_dict[accession_number] = {}\n",
    "\n",
    "# this dictionary will contain two keys, the sec header content, and a documents key.\n",
    "master_filings_dict[accession_number]['sec_header_content'] = {}\n",
    "master_filings_dict[accession_number]['filing_documents'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examing the SEC-Header Tag\n",
    "\n",
    "Honestly I would not want to scrape this it's a bunch text that isn't strucutred well. Also you probably can get this information from somewhere else and have it be in a much more structured format.However, there may be some information in this part that people want. Grab it first, and parse later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the sec-header tag, so we can store it in the master filing dictionary.\n",
    "sec_header_tag = soup.find('sec-header')\n",
    "\n",
    "# store the tag in the dictionary just as is.\n",
    "master_filings_dict[accession_number]['sec_header_content']['sec_header_code'] = sec_header_tag\n",
    "\n",
    "# display the sec header tag, so you can see how it looks.\n",
    "display(sec_header_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the documents\n",
    "\n",
    "Now the fun part, grabing all the documents. This actually isn't too bad, just do a find all with the `document` tag and loop through the results. You'll see how I get all the info I need from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize the dictionary that will house all of our documents\n",
    "master_document_dict = {}\n",
    "\n",
    "# find all the documents in the filing.\n",
    "for filing_document in soup.find_all('document'):\n",
    "    \n",
    "    # define the document type, found under the <type> tag, this will serve as our key for the dictionary.\n",
    "    document_id = filing_document.type.find(text=True, recursive=False).strip()\n",
    "    \n",
    "    # here are the other parts if you want them.\n",
    "    document_sequence = filing_document.sequence.find(text=True, recursive=False).strip()\n",
    "    document_filename = filing_document.filename.find(text=True, recursive=False).strip()\n",
    "    document_description = filing_document.description.find(text=True, recursive=False).strip()\n",
    "    \n",
    "    # initalize our document dictionary\n",
    "    master_document_dict[document_id] = {}\n",
    "    \n",
    "    # add the different parts, we parsed up above.\n",
    "    master_document_dict[document_id]['document_sequence'] = document_sequence\n",
    "    master_document_dict[document_id]['document_filename'] = document_filename\n",
    "    master_document_dict[document_id]['document_description'] = document_description\n",
    "    \n",
    "    # store the document itself, this portion extracts the HTML code. We will have to reparse it later.\n",
    "    master_document_dict[document_id]['document_code'] = filing_document.extract()\n",
    "    \n",
    "    \n",
    "\n",
    "    # grab the text portion of the document, this will be used to split the document into pages.\n",
    "    filing_doc_text = filing_document.find('text').extract()\n",
    "\n",
    "    \n",
    "    \n",
    "    # find all the thematic breaks, these help define page numbers and page breaks.\n",
    "    all_thematic_breaks = filing_doc_text.find_all('hr',{'width':'100%'})\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        THE FOLLOWING CODE IS OPTIONAL:\n",
    "        -------------------------------\n",
    "        \n",
    "        This portion will demonstrate how to parse the page number from each \"page\". Now I would only do this if you\n",
    "        want the ACTUAL page number on the document, if you don't need it then forget about it and just wait till the\n",
    "        next seciton.\n",
    "        \n",
    "        Additionally, some of the documents appear not to have page numbers when they should so there is no guarantee\n",
    "        that all the documents will be nice and organized.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # grab all the page numbers, first one is usually blank\n",
    "    all_page_numbers = [thematic_break.parent.parent.previous_sibling.previous_sibling.get_text(strip=True) \n",
    "                        for thematic_break in all_thematic_breaks]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "        If the above list comprehension doesn't make sense to you, here is how it would look as a regular loop.\n",
    "    \n",
    "        # define a list to house all the page numbers\n",
    "        all_page_numbers = []\n",
    "\n",
    "        # loop throuhg all the thematic breaks.\n",
    "        for thematic_break in all thematic_breaks:\n",
    "\n",
    "           # this would grab the page number tag.\n",
    "           page_number = thematic_break.parent.parent.previous_sibling.previous_sibling\n",
    "\n",
    "           # this would grab the page number text\n",
    "           page_number = page_number.get_text(strip=True)\n",
    "           \n",
    "           # store it in the list.\n",
    "           all_page_numbers.append(page_number)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # determine the number of pages, will be used for the upcoming if conditions.\n",
    "    length_of_page_numbers = len(all_page_numbers)\n",
    "    \n",
    "    # as long as there are numbers to change then proceed.\n",
    "    if length_of_page_numbers > 0:\n",
    "        \n",
    "        # grab the last number\n",
    "        previous_number = all_page_numbers[-1]\n",
    "        \n",
    "        # initalize a new list\n",
    "        all_page_numbers_cleaned = []\n",
    "        \n",
    "        # loop through the old list in reverse order.\n",
    "        for number in reversed(all_page_numbers):\n",
    "            \n",
    "            # if it's blank proceed to cleaning.\n",
    "            if number == '':\n",
    "                \n",
    "                # the tricky part, there are three scenarios.\n",
    "\n",
    "                # the previous one we looped was 0 or 1.\n",
    "                if previous_number == '1' or previous_number == '0':\n",
    "                    \n",
    "                    # in this case, it means this is a \"new section\", so restart at 0.\n",
    "                    all_page_numbers_cleaned.append(str(0))\n",
    "                    \n",
    "                    # reset the page number and the previous number.\n",
    "                    length_of_page_numbers = length_of_page_numbers - 1\n",
    "                    previous_number = '0'\n",
    "                \n",
    "                # the previous one we looped it wasn't either of those.\n",
    "                else:\n",
    "                    \n",
    "                    # if it was blank, take the current length, subtract 1, and add it to the list.\n",
    "                    all_page_numbers_cleaned.append(str(length_of_page_numbers - 1))\n",
    "                    \n",
    "                    # reset the page number and the previous number.\n",
    "                    length_of_page_numbers = length_of_page_numbers - 1\n",
    "                    previous_number = number\n",
    "\n",
    "            else:\n",
    "                \n",
    "                # add the number to the list.\n",
    "                all_page_numbers_cleaned.append(number)\n",
    "                \n",
    "                # reset the page number and the previous number.\n",
    "                length_of_page_numbers = length_of_page_numbers - 1\n",
    "                previous_number = number\n",
    "    else:\n",
    "        \n",
    "        # make sure that it has a page number even if there are none, just have it equal 0\n",
    "        all_page_numbers_cleaned = ['0']\n",
    "    \n",
    "    # have the page numbers be the cleaned ones, in reversed order.\n",
    "    all_page_numbers = list(reversed(all_page_numbers_cleaned))\n",
    "    \n",
    "    # store the page_numbers\n",
    "    master_document_dict[document_id]['page_numbers'] = all_page_numbers\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        -------------------------------\n",
    "          THE OPTIONAL CODE HAS ENDED\n",
    "        -------------------------------\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    # convert all thematic breaks to a string so it can be used for parsing\n",
    "    all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "    \n",
    "    # prep the document text for splitting, this means converting it to a string.\n",
    "    filing_doc_string = str(filing_doc_text)\n",
    "\n",
    "    \n",
    "    # handle the case where there are thematic breaks.\n",
    "    if len(all_thematic_breaks) > 0:\n",
    "    \n",
    "        # define the regex delimiter pattern, this would just be all of our thematic breaks.\n",
    "        regex_delimiter_pattern = '|'.join(map(re.escape, all_thematic_breaks))\n",
    "\n",
    "        # split the document along each thematic break.\n",
    "        split_filing_string = re.split(regex_delimiter_pattern, filing_doc_string)\n",
    "\n",
    "        # store the document itself\n",
    "        master_document_dict[document_id]['pages_code'] = split_filing_string\n",
    "\n",
    "    # handle the case where there are no thematic breaks.\n",
    "    elif len(all_thematic_breaks) == 0:\n",
    "\n",
    "        # handles so it will display correctly.\n",
    "        split_filing_string = all_thematic_breaks\n",
    "        \n",
    "        # store the document as is, since there are no thematic breaks. In other words, no splitting.\n",
    "        master_document_dict[document_id]['pages_code'] = [filing_doc_string]\n",
    "    \n",
    "\n",
    "    # display some information to the user.\n",
    "    print('-'*80)\n",
    "    print('The document {} was parsed.'.format(document_id))\n",
    "    print('There was {} page(s) found.'.format(len(all_page_numbers)))\n",
    "    print('There was {} thematic breaks(s) found.'.format(len(all_thematic_breaks)))\n",
    "    \n",
    "\n",
    "# store the documents in the master_filing_dictionary.\n",
    "master_filings_dict[accession_number]['filing_documents'] = master_document_dict\n",
    "\n",
    "print('-'*80)\n",
    "print('All the documents for filing {} were parsed and stored.'.format(accession_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Text\n",
    "\n",
    "Okay, so at this point we are in pretty good shape. We have all the documents, they've been split into their pages and have been stored in the dictionary. From here, we can proceed to further data cleaning and transformation. Now, I will add a little bit of a disclosure. Technically, we could have done some of this cleaning up above but for organization purpsoes I decided to just loop through everythin again and parse it that way.\n",
    "\n",
    "This would obviously, increase the amount of time it will take to run your code but it is easier to follow. I will provide a more \"optimized\" script that will do the cleaning up above and store that on GitHub. Another drawback to this method, is that I have to pass through all the code for each page back through BeautifulSoup. The reason why is because the tags, have been slightly messed up during the extraction and have to be fixed. I haven't run into too many hiccups with this but there is no guarantee that this will be the case for every filing type.\n",
    "\n",
    "You might be asking why I am doing this method if there are these drawbacks. Well, I find I have a little bit more control over the process when it comes to leveraging patterns in the HTML code. These patterns, will help us be able to find section headers, titles and other \"meaningful\" components of the page that will give us context as to what it contains. This wills serve to be useful down the road when we want to look for very specfic parts of a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first grab all the documents\n",
    "filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    # display some info to give status updates.\n",
    "    print('-'*80)\n",
    "    print('Pulling document {} for text normilzation.'.format(document))\n",
    "    \n",
    "    # grab all the pages for that document\n",
    "    document_pages = filing_documents[document_id]['pages_code']\n",
    "    \n",
    "    # page length\n",
    "    pages_length = len(filing_documents[document_id]['pages_code'])\n",
    "    \n",
    "    # initalize a dictionary that'll house our repaired html code for each page.\n",
    "    repaired_pages = {}\n",
    "    \n",
    "    # initalize a dictionary that'll house all the normalized text.\n",
    "    normalized_text = {}\n",
    "\n",
    "    # loop through each page in that document.\n",
    "    for index, page in enumerate(document_pages):\n",
    "        \n",
    "        # pass it through the parser. NOTE I AM USING THE HTML5 PARSER. YOU MUST USE THIS TO FIX BROKEN TAGS.\n",
    "        page_soup = BeautifulSoup(page,'html5')\n",
    "        \n",
    "        # grab all the text, notice I go to the BODY tag to do this\n",
    "        page_text = page_soup.html.body.get_text(' ',strip = True)\n",
    "        \n",
    "        # normalize the text, remove messy characters. Additionally, restore missing window characters.\n",
    "        page_text_norm = restore_windows_1252_characters(unicodedata.normalize('NFKD', page_text)) \n",
    "        \n",
    "        # Additional cleaning steps, removing double spaces, and new line breaks.\n",
    "        page_text_norm = page_text_norm.replace('  ', ' ').replace('\\n',' ')\n",
    "                \n",
    "        \n",
    "        '''\n",
    "            NOTES FROM UP ABOVE:\n",
    "            --------------------\n",
    "            \n",
    "            Remember up above, where I had some optional code. Well, this is where we add page numbers. If you notice\n",
    "            I simply take the index add 1 to it and with that we now have a page number. Now, this doesn't technically\n",
    "            follow the sections in each document but I don't think most people will care. Also we will see that we can\n",
    "            infer the sections from other parts.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # define the page number.\n",
    "        page_number = index + 1\n",
    "        \n",
    "        # add the normalized text to the list.\n",
    "        normalized_text[page_number] = page_text_norm\n",
    "        \n",
    "        # add the repaired html to the list. Also now we have a page number as the key.\n",
    "        repaired_pages[page_number] = page_soup\n",
    "    \n",
    "        # display a status to the user\n",
    "        print('Page {} of {} from document {} has had their text normalized.'.format(index + 1, \n",
    "                                                                                     pages_length, \n",
    "                                                                                     document))\n",
    "\n",
    "    # add the normalized text back to the document dictionary\n",
    "    filing_documents[document_id]['pages_normalized_text'] = normalized_text\n",
    "    \n",
    "    # add the repaired html code back to the document dictionary\n",
    "    filing_documents[document_id]['pages_code'] = repaired_pages\n",
    "    \n",
    "    # define the generated page numbers\n",
    "    gen_page_numbers = list(repaired_pages.keys())\n",
    "    \n",
    "    # add the page numbers we have.\n",
    "    filing_documents[document_id]['pages_numbers_generated'] = gen_page_numbers    \n",
    "    \n",
    "    # display a status to the user.\n",
    "    print('All the pages from document {} have been normalized.'.format(document_id))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Scraping, and Context Extraction\n",
    "\n",
    "Alright, so now we are really in a good spot. From here on out, most of what we need to do will be relatively straight forward. What I'll demonstrate next is how to get different parts of each page. Additionally, I'll show you strategies for helping to infer the \"context\" of each page. For example, is a page a signature page, an exhibit page, or a section page? This will help give you context and help you get to where you need to be when getting info.\n",
    "\n",
    "Also I'll demonstrate a strategy for how to search for key words on each page.\n",
    "\n",
    "Now, like I mentioned up above, this might not be the fastest way to do it but I am trying to layout this information in a logical format that flows from section to section. I am not concerned about speed at this point, I'm concerned about you following the steps I'm doing to clean my data. I will be posting a more \"optimized\" script on GitHub in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Search Words\n",
    "\n",
    "Let's define some words we might want to search for. These are just guesses and there is no guarantee that they will actually be the words we want. Unfortunately, you might have to read a few documents in order to determine the words that most appear in certain parts of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dict = {\n",
    "    \n",
    "    # these could possibly be words that help us find pages that discuss financial statements.\n",
    "    'financial_words':['liability', 'asset'],\n",
    "    \n",
    "    # these could possible be words that help us find sections that discuss administration topics.\n",
    "    'admin_words':['administration', 'government']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first grab all the documents\n",
    "filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    \n",
    "    ####################################\n",
    "    # THIS WILL HANDLE THE WORD SEARCH #\n",
    "    ####################################\n",
    "    \n",
    "    \n",
    "    # let's grab the normalized text in this example, since it's cleaned and easier to search\n",
    "    normalized_text_dict = filing_documents[document_id]['pages_normalized_text']  \n",
    "            \n",
    "    # initalize a dictionary to store all the tables we find.\n",
    "    matching_words_dict = {}\n",
    "    \n",
    "    # define the number of pages\n",
    "    page_length = len(normalized_text_dict)\n",
    "    \n",
    "    # loop through all the text\n",
    "    for page_num in normalized_text_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        normalized_page_text = normalized_text_dict[page_num]\n",
    "        \n",
    "        # each page is going to be checked, so let's have another dictionary that'll house each pages result.\n",
    "        matching_words_dict[page_num] = {}\n",
    "        \n",
    "        # loop through each word list in the search dictionary.\n",
    "        for search_list in search_dict:\n",
    "            \n",
    "            # grab the list of words.\n",
    "            list_of_words = search_dict[search_list]\n",
    "            \n",
    "            # lets see if any of the words are found\n",
    "            matching_words = [word for word in list_of_words if word in normalized_page_text]\n",
    "            \n",
    "            '''\n",
    "                Again, I know list comprehension might be hard to understand so I'll show you what the loop\n",
    "                looks like.\n",
    "                \n",
    "                # initalize a list of matching words.\n",
    "                matching_words = []\n",
    "                \n",
    "                # loop through the list of words.\n",
    "                for word in list_of_words:\n",
    "                \n",
    "                    # check to see if it's in the text\n",
    "                    if word in normalized_page_text:\n",
    "                        \n",
    "                        # if it is then add it to the list.\n",
    "                        matching_words.append(word)\n",
    "            '''\n",
    "            \n",
    "            # each page will have a set of results, list of words\n",
    "            matching_words_dict[page_num][search_list] = {}\n",
    "            \n",
    "            # let's add the list of words we search to the matching words dictionary first.\n",
    "            matching_words_dict[page_num][search_list]['list_of_words'] = list_of_words\n",
    "            \n",
    "            # next let's add the list of matchings words to the matching words dictionary.\n",
    "            matching_words_dict[page_num][search_list]['matches'] = matching_words\n",
    "            \n",
    "        \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} has been searched.'.format(page_num, page_length, document_id))\n",
    "    \n",
    "    \n",
    "    # display a status to the user.\n",
    "    print('-'*80)    \n",
    "    print('All the pages from document {} have been searched.'.format(document_id))    \n",
    "    \n",
    "    \n",
    "    ####################################\n",
    "    # THIS WILL HANDLE THE LINK SEARCH #\n",
    "    ####################################\n",
    "    \n",
    "    \n",
    "    # let's grab the all pages code.\n",
    "    pages_dict = filing_documents[document_id]['pages_code']  \n",
    "            \n",
    "    # initalize a dictionary to store all the anchors we find.\n",
    "    link_anchor_dict = {}\n",
    "    \n",
    "    # loop through each page\n",
    "    for page_num in pages_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        page_code = pages_dict[page_num]\n",
    "        \n",
    "        # find all the anchors in the page, that have the attribute 'name'\n",
    "        anchors_found = page_code.find_all('a',{'name':True})\n",
    "        \n",
    "        # number of anchors found\n",
    "        num_found = len(anchors_found)\n",
    "        \n",
    "        # each page is going to be checked, so let's have another dictionary that'll house all the anchors found.\n",
    "        link_anchor_dict[page_num]= {(anchor_id + 1): anchor for anchor_id, anchor in enumerate(anchors_found)}        \n",
    "    \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} contained {} anchors with names.'.format(page_num, \n",
    "                                                                                       page_length, \n",
    "                                                                                       document_id, \n",
    "                                                                                       num_found))\n",
    "    \n",
    "    # display a status to the user.  \n",
    "    print('All the pages from document {} have been scraped for anchors with names.'.format(document_id)) \n",
    "    print('-'*80)  \n",
    "    \n",
    "    \n",
    "    #####################################\n",
    "    # THIS WILL HANDLE THE TABLE SEARCH #\n",
    "    #####################################\n",
    "    \n",
    "         \n",
    "    # let's grab the all pages code.\n",
    "    pages_dict = filing_documents[document_id]['pages_code']  \n",
    "            \n",
    "    # initalize a dictionary to store matching words.\n",
    "    tables_dict = {}\n",
    "    \n",
    "    # loop through each page\n",
    "    for page_num in pages_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        page_code = pages_dict[page_num]\n",
    "        \n",
    "        # find all the tables\n",
    "        tables_found = page_code.find_all('table')\n",
    "        \n",
    "        # number of tables found\n",
    "        num_found = len(tables_found)\n",
    "        \n",
    "        # each page is going to be checked, so let's have another dictionary that'll house all the tables found.\n",
    "        tables_dict[page_num] = {(table_id + 1): table for table_id, table in enumerate(tables_found)}        \n",
    "    \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} contained {} tables.'.format(page_num, page_length, document_id, num_found))\n",
    "    \n",
    "    # display a status to the user.  \n",
    "    print('All the pages from document {} have been scraped for tables.'.format(document_id)) \n",
    "    print('-'*80)    \n",
    "    \n",
    "        \n",
    "    # let's add the matching words dict to the document.\n",
    "    filing_documents[document_id]['word_search'] = matching_words_dict  \n",
    "    \n",
    "    # let's add the matching tables dict to the document.\n",
    "    filing_documents[document_id]['table_search'] = tables_dict\n",
    "    \n",
    "    # let's add the matching anchors dict to the document.\n",
    "    filing_documents[document_id]['anchor_search'] = link_anchor_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Tables\n",
    "\n",
    "Now I know for some of you that you'll want to scrape the tables. I've provided a function you can use that will take your tables dictionary and parse each table to the best of it's ability. Unfortuantely this might not return perfect results but it's a good starting point from you to build off of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_table_dictionary(table_dictionary):\n",
    "    \n",
    "    # initalize a new dicitonary that'll house all your results\n",
    "    new_table_dictionary = {}\n",
    "    \n",
    "    if len(table_dictionary) != 0:\n",
    "\n",
    "        # loop through the dictionary\n",
    "        for table_id in table_dictionary:\n",
    "\n",
    "            # grab the table\n",
    "            table_html = table_dictionary[table_id]\n",
    "            \n",
    "            # grab all the rows.\n",
    "            table_rows = table_html.find_all('tr')\n",
    "            \n",
    "            # parse the table, first loop through the rows, then each element, and then parse each element.\n",
    "            parsed_table = [\n",
    "                [element.get_text(strip=True) for element in row.find_all('td')]\n",
    "                for row in table_rows\n",
    "            ]\n",
    "            \n",
    "            # keep the original just to be safe.\n",
    "            new_table_dictionary[table_id]['original_table'] = table_html\n",
    "            \n",
    "            # add the new parsed table.\n",
    "            new_table_dictionary[table_id]['parsed_table'] = parsed_table\n",
    "            \n",
    "            # here some additional steps you can take to clean up the data - Removing '$'.\n",
    "            parsed_table_cleaned = [\n",
    "                [element for element in row if element != '$']\n",
    "                for row in parsed_table\n",
    "            ]\n",
    "            \n",
    "            # here some additional steps you can take to clean up the data - Removing Blanks.\n",
    "            parsed_table_cleaned = [\n",
    "                [element for element in row if element != None]\n",
    "                for row in parsed_table_cleaned.\n",
    "            ]\n",
    "\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # if there are no tables then just have the id equal NONE\n",
    "        new_table_dictionary[1]['original_table'] = None\n",
    "        new_table_dictionary[1]['parsed_table'] = None\n",
    "        \n",
    "    return new_table_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Searches\n",
    "\n",
    "There are some extra strategies we can take in order to find different parts of the page. For example, we might want the text that is centered over the page. This might give us context as to what is in the page and serve as a header. However, in order to do this search we will have to define some functions we need to pass through to beautfiul soup.\n",
    "\n",
    "***\n",
    "### Searching For Centered Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_centered_headers(tag):\n",
    "\n",
    "    # easy way to end early is check if the 'align' keet is in attributes.\n",
    "    if 'align' not in tag.attrs:\n",
    "        return\n",
    "    \n",
    "    # define the criteria.\n",
    "    criteria1 = tag.name == 'p'                # I want the tag to be name of 'p'\n",
    "    criteria2 = tag.parent.name != 'td'        # I want the parent tag NOT to be named 'td'\n",
    "    criteria3 = tag['align'] == 'center'       # I want the 'align' attribute to be labeled 'center'.\n",
    "    \n",
    "    # if it matches all the criteria then return the text.\n",
    "    if criteria1 and criteria2 and criteria3:         \n",
    "        return tag.get_text(strip = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search For Bolded Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_bolded_tags(tag):\n",
    "    \n",
    "    # define the criteria.\n",
    "    criteria1 = tag.name == 'b'                # I want the tag to be name of 'p'\n",
    "    criteria2 = tag.parent.name != 'td'        # I want the parent tag NOT to be named 'td'\n",
    "    \n",
    "    # if it matches all the criteria then return the text.\n",
    "    if criteria1 and criteria2:         \n",
    "        return tag.get_text(strip = True).replace('\\n',' ')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Using Custom Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first grab all the documents\n",
    "filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:   \n",
    "    \n",
    "    # let's grab the all pages code.\n",
    "    pages_dict = filing_documents[document_id]['pages_code']  \n",
    "            \n",
    "    # initalize a dictionary to store all the anchors we find.\n",
    "    centered_headers_dict = {}\n",
    "    \n",
    "    # loop through each page\n",
    "    for page_num in pages_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        page_code = pages_dict[page_num]\n",
    "        \n",
    "        # find all the anchors in the page, that have the attribute 'name'\n",
    "        centered_headers_found = page_code.find_all(search_for_centered_headers)\n",
    "        \n",
    "        # number of anchors found\n",
    "        num_found = len(centered_headers_found)\n",
    "   \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} contained {} centered headers.'.format(page_num, \n",
    "                                                                                     page_length, \n",
    "                                                                                     document_id, \n",
    "                                                                                     num_found))\n",
    "    \n",
    "    # display a status to the user.  \n",
    "    print('All the pages from document {} have been scraped for centered headers.'.format(document_id)) \n",
    "    print('-'*80)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
